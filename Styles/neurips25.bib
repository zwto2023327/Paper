@article{radenovic2022neural,
  title={Neural basis models for interpretability},
  author={Radenovic, Filip and Dubey, Abhimanyu and Mahajan, Dhruv},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8414--8426},
  year={2022}
}
@article{gao2023not,
  title={Not all samples are born equal: Towards effective clean-label backdoor attacks},
  author={Gao, Yinghua and Li, Yiming and Zhu, Linghui and Wu, Dongxian and Jiang, Yong and Xia, Shu-Tao},
  journal={Pattern Recognition},
  volume={139},
  pages={109512},
  year={2023},
  publisher={Elsevier}
}
@article{li2023explore,
  title={Explore the effect of data selection on poison efficiency in backdoor attacks},
  author={Li, Ziqiang and Xia, Pengfei and Sun, Hong and Zeng, Yueqi and Zhang, Wei and Li, Bin},
  journal={arXiv preprint arXiv:2310.09744},
  year={2023}
}
@article{li2024proxy,
  title={A Proxy Attack-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks},
  author={Li, Ziqiang and Sun, Hong and Xia, Pengfei and Xia, Beihao and Rui, Xue and Zhang, Wei and Guo, Qinglang and Fu, Zhangjie and Li, Bin},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2024},
  publisher={IEEE}
}
@article{land1971lightness,
  title={Lightness and retinex theory},
  author={Land, Edwin H and McCann, John J},
  journal={Journal of the Optical society of America},
  volume={61},
  number={1},
  pages={1--11},
  year={1971},
  publisher={Optical Society of America}
}
@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}
@article{chen2017targeted,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}
@article{turner2019label,
  title={Label-consistent backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  journal={arXiv preprint arXiv:1912.02771},
  year={2019}
}
@article{chou2023villandiffusion,
  title={Villandiffusion: A unified backdoor attack framework for diffusion models},
  author={Chou, Sheng-Yen and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={33912--33964},
  year={2023}
}
@article{wang2024invisible,
  title={Invisible black-box backdoor attack against deep cross-modal hashing retrieval},
  author={Wang, Tianshi and Li, Fengling and Zhu, Lei and Li, Jingjing and Zhang, Zheng and Shen, Heng Tao},
  journal={ACM Transactions on Information Systems},
  volume={42},
  number={4},
  pages={1--27},
  year={2024},
  publisher={ACM New York, NY}
}
@article{hayase2022few,
  title={Few-shot backdoor attacks via neural tangent kernels},
  author={Hayase, Jonathan and Oh, Sewoong},
  journal={arXiv preprint arXiv:2210.05929},
  year={2022}
}
@article{zhu2023boosting,
  title={Boosting backdoor attack with a learnable poisoning sample selection strategy},
  author={Zhu, Zihao and Zhang, Mingda and Wei, Shaokui and Shen, Li and Fan, Yanbo and Wu, Baoyuan},
  journal={arXiv preprint arXiv:2307.07328},
  year={2023}
}
@article{xun2024minimalism,
  title={Minimalism is King! High-Frequency Energy-based Screening for Data-Efficient Backdoor Attacks},
  author={Xun, Yuan and Jia, Xiaojun and Gu, Jindong and Liu, Xinwei and Guo, Qing and Cao, Xiaochun},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2024},
  publisher={IEEE}
}
@article{wenger2022finding,
  title={Finding naturally occurring physical backdoors in image datasets},
  author={Wenger, Emily and Bhattacharjee, Roma and Bhagoji, Arjun Nitin and Passananti, Josephine and Andere, Emilio and Zheng, Heather and Zhao, Ben},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22103--22116},
  year={2022}
}
@article{li2024invisible,
  title={Invisible backdoor attacks on diffusion models},
  author={Li, Sen and Ma, Junchi and Cheng, Minhao},
  journal={arXiv preprint arXiv:2406.00816},
  year={2024}
}
@article{zhao2024exploring,
  title={Exploring clean label backdoor attacks and defense in language models},
  author={Zhao, Shuai and Tuan, Luu Anh and Fu, Jie and Wen, Jinming and Luo, Weiqi},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}
@article{gao2024backdoor,
  title={Backdoor attack with sparse and invisible trigger},
  author={Gao, Yinghua and Li, Yiming and Gong, Xueluan and Li, Zhifeng and Xia, Shu-Tao and Wang, Qian},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2024},
  publisher={IEEE}
}
@article{liu2023backdoor,
  title={Backdoor attacks against dataset distillation},
  author={Liu, Yugeng and Li, Zheng and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2301.01197},
  year={2023}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{huynh2024combat,
  title={Combat: Alternated training for effective clean-label backdoor attacks},
  author={Huynh, Tran and Nguyen, Dang and Pham, Tung and Tran, Anh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={3},
  pages={2436--2444},
  year={2024}
}
@inproceedings{chen2023practical,
  title={A practical clean-label backdoor attack with limited information in vertical federated learning},
  author={Chen, Peng and Yang, Jirui and Lin, Junxiong and Lu, Zhihui and Duan, Qiang and Chai, Hongfeng},
  booktitle={2023 IEEE International Conference on Data Mining (ICDM)},
  pages={41--50},
  year={2023},
  organization={IEEE}
}
@inproceedings{wu2023computation,
  title={Computation and data efficient backdoor attacks},
  author={Wu, Yutong and Han, Xingshuo and Qiu, Han and Zhang, Tianwei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4805--4814},
  year={2023}
}
@inproceedings{li20233dfed,
  title={3dfed: Adaptive and extensible framework for covert backdoor attack in federated learning},
  author={Li, Haoyang and Ye, Qingqing and Hu, Haibo and Li, Jin and Wang, Leixia and Fang, Chengfang and Shi, Jie},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={1893--1907},
  year={2023},
  organization={IEEE}
}
@inproceedings{han2024backdooring,
  title={Backdooring multimodal learning},
  author={Han, Xingshuo and Wu, Yutong and Zhang, Qingjie and Zhou, Yuan and Xu, Yuan and Qiu, Han and Xu, Guowen and Zhang, Tianwei},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  pages={3385--3403},
  year={2024},
  organization={IEEE}
}
@inproceedings{qiu2024belt,
  title={BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting},
  author={Qiu, Huming and Sun, Junjie and Zhang, Mi and Pan, Xudong and Yang, Min},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  pages={2124--2141},
  year={2024},
  organization={IEEE}
}
@inproceedings{bai2022hardly,
  title={Hardly perceptible trojan attack against neural networks with bit flips},
  author={Bai, Jiawang and Gao, Kuofeng and Gong, Dihong and Xia, Shu-Tao and Li, Zhifeng and Liu, Wei},
  booktitle={European Conference on Computer Vision},
  pages={104--121},
  year={2022},
  organization={Springer}
}
@inproceedings{lin2020composite,
  title={Composite backdoor attack for deep neural network by mixing existing benign features},
  author={Lin, Junyu and Xu, Lei and Liu, Yingqi and Zhang, Xiangyu},
  booktitle={Proceedings of the 2020 ACM SIGSAC conference on computer and communications security},
  pages={113--131},
  year={2020}
}
@inproceedings{qi2022towards,
  title={Towards practical deployment-stage backdoor attack on deep neural networks},
  author={Qi, Xiangyu and Xie, Tinghao and Pan, Ruizhe and Zhu, Jifeng and Yang, Yong and Bu, Kai},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13347--13357},
  year={2022}
}
@inproceedings{khaddaj2023rethinking,
  title={Rethinking backdoor attacks},
  author={Khaddaj, Alaa and Leclerc, Guillaume and Makelov, Aleksandar and Georgiev, Kristian and Salman, Hadi and Ilyas, Andrew and Madry, Aleksander},
  booktitle={International Conference on Machine Learning},
  pages={16216--16236},
  year={2023},
  organization={PMLR}
}
@inproceedings{huang2024uba,
  title={$\{$UBA-Inf$\}$: Unlearning Activated Backdoor Attack with $\{$Influence-Driven$\}$ Camouflage},
  author={Huang, Zirui and Mao, Yunlong and Zhong, Sheng},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={4211--4228},
  year={2024}
}
@inproceedings{liu2020reflection,
  title={Reflection backdoor: A natural backdoor attack on deep neural networks},
  author={Liu, Yunfei and Ma, Xingjun and Bailey, James and Lu, Feng},
  booktitle={Computer vision--ECCV 2020: 16th European conference, Glasgow, UK, August 23--28, 2020, proceedings, part X 16},
  pages={182--199},
  year={2020},
  organization={Springer}
}
@inproceedings{barni2019new,
  title={A new backdoor attack in cnns by training set corruption without label poisoning},
  author={Barni, Mauro and Kallas, Kassem and Tondi, Benedetta},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)},
  pages={101--105},
  year={2019},
  organization={IEEE}
}
@inproceedings{wang2025not,
  title={Not All Benignware Are Alike: Enhancing Clean-Label Attacks on Malware Classifiers},
  author={Wang, Xutong and Feng, Yun and Bi, Bingsheng and Cao, Yaqin and Jin, Ze and Liu, Xinyu and Liu, Yuling and Li, Yunpeng},
  booktitle={THE WEB CONFERENCE 2025}
}
@inproceedings{hungwicked,
  title={Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks},
  author={Hung-Quang, Nguyen and Nguyen, Ngoc-Hieu and Nguyen-Tang, Thanh and Wong, Kok-Seng and Thanh-Tung, Hoang and Doan, Khoa D and others},
  booktitle={The Thirteenth International Conference on Learning Representations}
}
@inproceedings{wang2022bppattack,
  title={Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning},
  author={Wang, Zhenting and Zhai, Juan and Ma, Shiqing},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15074--15084},
  year={2022}
}
@inproceedings {287378,
author = {Peizhuo Lv and Chang Yue and Ruigang Liang and Yunfei Yang and Shengzhi Zhang and Hualong Ma and Kai Chen},
title = {A Data-free Backdoor Injection Approach in Neural Networks},
booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
year = {2023},
isbn = {978-1-939133-37-3},
address = {Anaheim, CA},
pages = {2671--2688},
url = {https://www.usenix.org/conference/usenixsecurity23/presentation/lv},
publisher = {USENIX Association},
month = aug
}
@inproceedings{10.1145/3576915.3616617,
author = {Zeng, Yi and Pan, Minzhou and Just, Hoang Anh and Lyu, Lingjuan and Qiu, Meikang and Jia, Ruoxi},
title = {Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3616617},
doi = {10.1145/3576915.3616617},
abstract = {Backdoor attacks introduce manipulated data into a machine learning model's training set, causing the model to misclassify inputs with a trigger during testing to achieve a desired outcome by the attacker. For backdoor attacks to bypass human inspection, it is essential that the injected data appear to be correctly labeled. The attacks with such property are often referred to as "clean-label attacks." The success of current clean-label backdoor methods largely depends on access to the complete training set. Yet, accessing the complete dataset is often challenging or unfeasible since it frequently comes from varied, independent sources, like images from distinct users. It remains a question of whether backdoor attacks still present real threats.In this paper, we provide an affirmative answer to this question by designing an algorithm to launch clean-label backdoor attacks using only samples from the target class and public out-of-distribution data. By inserting carefully crafted malicious examples totaling less than 0.5\% of the target class size and 0.05\% of the full training set size, we can manipulate the model to misclassify arbitrary inputs into the target class when they contain the backdoor trigger. Importantly, the trained poisoned model retains high accuracy for regular test samples without the trigger, as if the model is trained on untainted data. Our technique is consistently effective across various datasets, models, and even when the trigger is injected into the physical world.We explore the space of defenses and find that Narcissus can evade the latest state-of-the-art defenses in their vanilla form or after a simple adaptation. We analyze the effectiveness of our attack - the synthesized Narcissus trigger contains durable features as persistent as the original target class features. Attempts to remove the trigger inevitably hurt model accuracy first.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {771â€“785},
numpages = {15},
keywords = {ai security, backdoor attack, clean-label attack},
location = {Copenhagen, Denmark},
series = {CCS '23}
}
@article{doan2021backdoor,
  title={Backdoor attack with imperceptible input and latent modification},
  author={Doan, Khoa and Lao, Yingjie and Li, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18944--18957},
  year={2021}
}